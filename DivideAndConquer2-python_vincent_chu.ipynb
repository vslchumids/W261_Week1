{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name              : Vincent Chu\n",
    "\n",
    "Email             : vslchu@gmail.com\n",
    "\n",
    "Class Name        : W261\n",
    "\n",
    "Session           : 2\n",
    "\n",
    "Week Number       : 1\n",
    "\n",
    "Date of submission: 5/10/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook provides a poor man Hadoop through command-line and python. Please insert the python code by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Please note that some minor changes have been made to this script other than the code generation logic of the Map and Reduce Python scripts.  This is due to the fact that I am running unix commands with Babun running on top of Windows 7.  As an example, I needed to remove carriage return characters in the pGrepCount.sh script that was generated by this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2]\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    #Please insert your code\n",
    "    \n",
    "    # Initialize cumulated count of the find word\n",
    "    cum_findword_count = 0\n",
    "    for line in myfile:\n",
    "        # Remove whitespaces at start and end of each line\n",
    "        line = line.strip()\n",
    "        # Split the each line into individual words\n",
    "        words = line.split()\n",
    "\n",
    "        # Initialize count of the find word in the current line\n",
    "        findword_count = 0\n",
    "        for word in words: \n",
    "            # Strip the incoming word from the file from all common \n",
    "            # punctuations, symbols and white spaces, and then compare \n",
    "            # to see if the stripped version matches the findword.\n",
    "            # Also, both the incoming words and the findword are \n",
    "            # transformed to all CAPS gfor comparison to avoid any \n",
    "            # differences only due to case.\n",
    "            if word.strip(\"'\\\",.?!@#$%^&*():;<>|{}[]\\/~ \\t\\n\\r\").upper() == findword.upper():\n",
    "                findword_count += 1            \n",
    "        cum_findword_count += findword_count\n",
    "    \n",
    "# print cumulated count of the findword  \n",
    "print cum_findword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "temp_int = 0\n",
    "for line in sys.stdin:\n",
    "#Please insert your code\n",
    "    \n",
    "    # convert each line, which contains just a single number, \n",
    "    # into integer value in preparation of being added to the \n",
    "    # cumulated sum\n",
    "    temp_int = int(line.strip())\n",
    "    sum += temp_int\n",
    "    \n",
    "# print sum of count of each \"line\" from sys.stdin \n",
    "sys.stdout.write(str(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Please note that I had to add the \"python\" command explicity to run the mapper.py and reducer.py scripts since I am running Unix commands using Babun installed on Windows 7.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pGrepCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pGrepCount.sh\n",
    "ORIGINAL_FILE=$1\n",
    "FIND_WORD=$2\n",
    "BLOCK_SIZE=$3\n",
    "CHUNK_FILE_PREFIX=$ORIGINAL_FILE.split\n",
    "SORTED_CHUNK_FILES=$CHUNK_FILE_PREFIX*.sorted\n",
    "usage()\n",
    "{\n",
    "    echo Parallel grep\n",
    "    echo usage: pGrepCount filename word chuncksize\n",
    "    echo greps file file1 in $ORIGINAL_FILE and counts the number of lines\n",
    "    echo Note: file1 will be split in chunks up to $ BLOCK_SIZE chunks each\n",
    "    echo $FIND_WORD each chunk will be grepCounted in parallel\n",
    "}\n",
    "#Splitting $ORIGINAL_FILE INTO CHUNKS\n",
    "split -b $BLOCK_SIZE $ORIGINAL_FILE $CHUNK_FILE_PREFIX\n",
    "#DISTRIBUTE\n",
    "for file in $CHUNK_FILE_PREFIX*\n",
    "do\n",
    "    #grep -i $FIND_WORD $file|wc -l >$file.intermediateCount &\n",
    "    python mapper.py $FIND_WORD $file >$file.intermediateCount &\n",
    "done\n",
    "wait\n",
    "#MERGEING INTERMEDIATE COUNT CAN TAKE THE FIRST COLUMN AND TOTOL...\n",
    "#numOfInstances=$(cat *.intermediateCount | cut -f 1 | paste -sd+ - |bc)\n",
    "numOfInstances=$(cat *.intermediateCount | python reducer.py)\n",
    "echo \"found [$numOfInstances] [$FIND_WORD] in the file [$ORIGINAL_FILE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the file\n",
    "\n",
    "*** Please note that I had to add a command to remove carriage returns from the pGrepCount.sh file since I am running these unix commands using Babun installed on Windows 7.  This carriage return issue is something that I previously encountered in W205, so applying the same resolution here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To remove carriage returns added from Windows system.\n",
    "# Final script is pGrepCount_v2.sh.\n",
    "!cat pGrepCount.sh | tr -d \"\\r\" > pGrepCount_v2.sh\n",
    "\n",
    "# Change access previlege\n",
    "!chmod a+x pGrepCount_v2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: pGrepCount filename word chuncksize\n",
    "\n",
    "*** Please note that .sh files need to be run with bash given that I am running unix commands using Babun installed on Windows 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [59] [COPYRIGHT] in the file [License.txt]\n"
     ]
    }
   ],
   "source": [
    "# Run pGrepCount_v2.sh on License.txt\n",
    "!bash pGrepCount_v2.sh License.txt COPYRIGHT 4k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mapper.py logic actually did better than the purely Unix-based poor man's map-reduce, thanks to the logic added to strip all the leading and trailing punctuations and symbols.  There are indeed 59 matches of the word COPYRIGHT in the License.txt file.  The purely Unix-based version reported only 57, most likely due to the quotes that were attached to 2 of the occurrences of the word COPYRIGHT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
